dblink : { 
    // Path to data files
    // Must have header row (column names). The format is inferred from the file extension.
    dataPath : "/data/neil/datasets/RLdata10000.csv"
    
    // String representation of a missing value
    nullValue : "NA"
    
    // Define some distortion hyperparameters (referenced below)
    lowDistortion : {alpha : 10.0, beta : 1000.0}

    // Define similarity functions (referenced below)
    constSimFn : {
        name : "ConstantSimilarityFn",
    }

    levSimFn : {
        name : "LevenshteinSimilarityFn",
        properties : {
            threshold : 7.0
            maxSimilarity : 10.0
        }
    }

    // Specify properties of the attributes (columns) used for matching
    matchingAttributes : [
        {name : "by", similarityFunction : ${dblink.constSimFn}, distortionPrior : ${dblink.lowDistortion}},
        {name : "bm", similarityFunction : ${dblink.constSimFn}, distortionPrior : ${dblink.lowDistortion}},
        {name : "bd", similarityFunction : ${dblink.constSimFn}, distortionPrior : ${dblink.lowDistortion}},
        {name : "fname_c1", similarityFunction : ${dblink.levSimFn}, distortionPrior : ${dblink.lowDistortion}},
        {name : "lname_c1", similarityFunction : ${dblink.levSimFn}, distortionPrior : ${dblink.lowDistortion}}
    ]
    
    // Specify which attributes (columns) contain identifiers
    identifierAttributes : {
        record : "rec_id",
        // file : null, // not needed since this data set is only a single file
        entity : "ent_id" // optional
    }

    randomSeed : 319158
    expectedMaxClusterSize : 10
    
    // Specify partitioner
    partitioner : {
        name : "KDTreePartitioner",
        properties : {
            numLevels : 1, // a value of zero means no partitioning
            matchingAttributes : ["fname_c1"] // cycle through matching attributes in this order when constructing the tree
        }
    }

    // Path to save Markov chain and full state (for resuming MCMC) + summaries
    outputPath : "/data/neil/jcgs/RLdata10000_2partitions_PCG-I"
    
    // Path to save Spark checkpoints
    checkpointPath : "/scratch/neil/dblink-checkpoints"
    
    // Steps to be performed (in order)
    steps : [
        {name : "sample", parameters : {
            sampleSize : 10000,
            burninInterval : 0,
            thinningInterval : 10,
            resume : false,
            sampler : "PCG-I"
        }},
        {name : "summarize", parameters : {
            lowerIterationCutoff : 0,
            quantities : ["cluster-size-distribution", "partition-sizes"]
        }},
        {name : "evaluate", parameters : {
            lowerIterationCutoff : 10000,
            metrics : ["pairwise", "cluster"], 
            useExistingSMPC : false
        }}
    ]
}
