Data settings
-------------
  * Using data files located at 's3://d-blink/datasets/proc_nltcs.csv'
  * The record identifier attribute is 'REC'
  * The file identifier attribute is 'FILE'
  * The entity identifier attribute is 'SEQ'
  * The matching attributes are 'SEX', 'DOB_DAY', 'DOB_MONTH', 'DOB_YEAR', 'STATE', 'REGOFF'

Hyperparameter settings
-----------------------
  * 'SEX' (id=0) with ConstantSimilarityFn and BetaShapeParameters(alpha=57.077, beta=5707.7)
  * 'DOB_DAY' (id=1) with ConstantSimilarityFn and BetaShapeParameters(alpha=57.077, beta=5707.7)
  * 'DOB_MONTH' (id=2) with ConstantSimilarityFn and BetaShapeParameters(alpha=57.077, beta=5707.7)
  * 'DOB_YEAR' (id=3) with ConstantSimilarityFn and BetaShapeParameters(alpha=57.077, beta=5707.7)
  * 'STATE' (id=4) with ConstantSimilarityFn and BetaShapeParameters(alpha=57.077, beta=5707.7)
  * 'REGOFF' (id=5) with ConstantSimilarityFn and BetaShapeParameters(alpha=57.077, beta=5707.7)

Partition function settings
---------------------------
  * KDTreePartitioner(numLevels=1, attributeIds=[2,1,3,4])

Project settings
----------------
  * Using randomSeed=319158
  * Using expectedMaxClusterSize=10
  * Saving Markov chain and complete final state to 'hdfs:///dblink-output/'
  * Saving Spark checkpoints to 'hdfs:///dblink-checkpoints/'

Scheduled steps
---------------
  * SampleStep: Evolving the chain from new initial state with sampleSize=1000, burninInterval=0, thinningInterval=10 and sampler=PCG-I
  * SummarizeStep: Calculating summary quantities {'cluster-size-distribution', 'partition-sizes'} along the chain for iterations >= 0
  * CopyFilesStep: Copying {cluster-size-distribution.csv, partition-sizes.csv, diagnostics.csv, run.txt} to destination s3://d-blink/NLTCS_2partitions_PCG-I/